[文章链接](https://github.com/YiruWangYuri/Paper-Reading/blob/master/09-2016ECCV-FGR-Fast%20Global%20Registration/09.0.pdf)

[ppt链接](https://github.com/YiruWangYuri/Paper-Reading/blob/master/09-2016ECCV-FGR-Fast%20Global%20Registration/09.2%20Slices.pdf)

[code链接](https://github.com/intel-isl/FastGlobalRegistration)

[文章主页](http://vladlen.info/publications/fast-global-registration/)

### 本文作者来自因特尔实验室，这篇文章的方法和效果还算是practical的，本文亮点在于优化的方式是guarantee nonconvex，逐步改变鲁棒函数的参数$\mu$

**Key idea:** a图是鲁棒的loss function，b图是带入目标函数之后的示意图，其实$\mu$越小，得出的结果越准确，不要看$\mu=16$的时候，似乎解也可接受，真实情况下可远不是这样子，即$\mu$很大，会偏离解的。而在代码中，由于作者对点集进行了归一化处理，所以$\mu$的选取从1开始的（1是点云的坐标尺度），1—0.02，这样的下降程度。个人认为，$\mu$的初始值选取需要更大一点（up to the scale），然后使得目标函数对初始值不敏感，并一步步减小$\mu$，来获得一个令人满意的solution。


1. 做工程的人，当然从比较practical的方式：RANSAC+ICP出发，作者认为这种方式的消耗在于inner loop中反复需要执行寻找最近邻，这样的开销不小：RANSAC里应该不用寻找最近邻，ICP是要的。同时，点云配准这个问题，把它变成了RANSAC+ICPrefine的方式，并不elegant：我认为，ICP本身就可以直接解决配准问题，为什么前面要加RANSAC，是为了提供良好的初始值；RANSAC本身就可以解决配准问题，为什么后面要加ICP，是为了更好的配准精度。
2. 作者提出Fast Global Registration，这里的Global并不是指数值优化问题中的全局最优解，而且该配准是有对应关系的配准。
    + 采用的配准框架是一个优化问题框架，利用鲁棒损失函数来减少outliers的影响，而非RANSAC+ICP模式。
    + 优化方法采有逐渐减小鲁棒损失函数中的$\mu$来完成，使得初始值可以任意，即尽管目标函数高度非凸，也可以得到良好的收敛效果，$\mu$大的时候，收敛域大，但结果不准，随着$\mu$的减小，引导目标函数来到全局最优的solution附近。
    + 配准采用FPHF特征匹配来提供对应关系，inner loop中不需要寻找最近点：其实这是很自然的事情，一个有对应关系的配准+outlier的污染==>鲁棒估计问题，优化目标函数最小，又由于对应关系已知，尽管对应关系有对有错，也不再需要寻找最近点，因为我的目标是在当前不可信又有点可信的对应关系下，寻找误差最小。
3. 作者在review的时候提到BnB类的global registration，这里的global（Go-icp，全局最优解）和它的globa不一样的，有点misleading。另外对比实验中，也和Go-icp做了对比，我觉得这有点不妥，Go-icp是没有对应关系的配准，所以inner loop要用最近邻来确定对应关系。而有对应关系的配准难度要比没有对应关系的配准难度更小，当然我也并不认为，将Go-icp修改为有对应关系的框架后，速度效果上会比本文更好。
4. 算法实现
    1. 目标函数的构建

    整合以后，$E(T)$的分子分母上都有$T$，这样的优化很难完成。因此作者使用Black-Rangarajan duality，引入$L={l_{p,q}}$。交替优化$L,T$。
    构造公式(3)：其目的为，$L$已知的情况下，$E(T,L)$与原始的目标函数越像越好，其目的一致。$L$已知的情况，即$\frac{\partial E}{\partial l}=0$的解，带入公式3后，与目标函数目的一致。将公式4，6，带入3中，可得$E = \frac{\mu^2 {\left \| p-Tq \right \|}^2+\mu {\left \| p-Tq \right \|}^4}{(\mu+{\left \| p-Tq \right \|}^2)^2} $，**代入之后和目标函数并不完全一样**，但其目的是否一致，我又不会证明。

    2. 前面已经隐含着，求优化$L$是求导，那么$L$固定时，如果优化$T$？
    显然这是个加权最小二乘问题，作者说这个问题可以有解析解，封闭解，但是对于多个面的联合配准来说，得不到封闭解，因此采用高斯牛顿法进行非线性最小二乘优化。以下很丑的扫描公式为我的推导。
    



3. 算法流程
    作者归一化了数据
    利用FPFG描述子寻找对应关系，其中也利用了最近邻，然后给出了个tuple策略，理解为两对点间距离比较接近，才会被进一步确定为潜在的对应点。
    交替优化$L,T$，计算L的时候，每4次迭代之后，$\mu$缩小。
    收敛：程序里给的是到达一定迭代次数停止或者是$\mu$缩小到某个阈值停止。





